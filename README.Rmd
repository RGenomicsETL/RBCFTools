---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# RBCFTools

<!-- badges: start -->
[![R-CMD-check](https://github.com/RGenomicsETL/RBCFTools/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/RGenomicsETL/RBCFTools/actions/workflows/R-CMD-check.yaml)
[![R-universe version](https://RGenomicsETL.r-universe.dev/RBCFTools/badges/version)](https://RGenomicsETL.r-universe.dev/RBCFTools)
<!-- badges: end -->

RBCFTools provides R bindings to [bcftools](https://github.com/samtools/bcftools) and [htslib](https://github.com/samtools/htslib), the standard tools for reading and manipulating VCF/BCF files. The package bundles these libraries and command-line tools (bcftools, bgzip, tabix), so no external installation is required. When compiled with libcurl, remote file access from S3, GCS, and HTTP URLs is supported. The package also includes experimental support for streaming VCF/BCF to Apache Arrow (IPC) format via [nanoarrow](https://arrow.apache.org/nanoarrow/), with export to Parquet format using [duckdb](https://duckdb.org/) via either the  [duckdb nanoarrow extension](https://duckdb.org/community_extensions/extensions/nanoarrow.html) or a [`bcf_reader`](inst/duckdb_bcf_reader_extension/) extension bundled in this package.

## Installation

You can install the development version of RBCFTools from [GitHub](https://github.com/RGenomicsETL/RBCFTools) for unix-alikes (we do not support windows)
 
```r
install.packages('RBCFTools', repos = c('https://rgenomicsetl.r-universe.dev', 'https://cloud.r-project.org'))
```

## Version Information

```{r versions}
library(RBCFTools)

# Get library versions
bcftools_version()
htslib_version()
```

## Tool Paths

RBCFTools bundles bcftools and htslib command-line tools. Use the path functions to locate the executables

```{r paths}
bcftools_path()
bgzip_path()
tabix_path()
# List all available tools
bcftools_tools()
htslib_tools()
```

## Capabilities

Check which features were compiled into htslib

```{r capabilities}
# Get all capabilities as a named list
htslib_capabilities()

# Human-readable feature string
htslib_feature_string()
```

### Feature Constants

Use `HTS_FEATURE_*` constants to check for specific features
 
```{r features}
# Check individual features
htslib_has_feature(HTS_FEATURE_LIBCURL)  # Remote file access via libcurl
htslib_has_feature(HTS_FEATURE_S3)       
htslib_has_feature(HTS_FEATURE_GCS)      # Google Cloud Storage
htslib_has_feature(HTS_FEATURE_LIBDEFLATE)
htslib_has_feature(HTS_FEATURE_LZMA)
htslib_has_feature(HTS_FEATURE_BZIP2)
```

These are useful for conditionally enabling features in your code.

## Example Query Remote VCF from S3 using bcftools

With libcurl support, bcftools can directly query remote files. Here we count variants in a small region from the 1000 Genomes cohort VCF on S3:

```{r s3-example, eval=TRUE}
# Setup environment for remote file access (S3/GCS)
setup_hts_env()

# Build S3 URL for 1000 Genomes cohort VCF
s3_base <- "s3://1000genomes-dragen-v3.7.6/data/cohorts/"
s3_path <- "gvcf-genotyper-dragen-3.7.6/hg19/3202-samples-cohort/"
s3_vcf_file <- "3202_samples_cohort_gg_chr22.vcf.gz"
s3_vcf_uri <- paste0(s3_base, s3_path, s3_vcf_file)

# Query a small region (chr22:20000000-20100000) and count variants
result <- system2(
  bcftools_path(),
  args = c("view", "-H", "-r", "chr22:20000000-20100000", s3_vcf_uri),
  stdout = TRUE,
  stderr = FALSE
)
length(result)
```


## (Experimental) VCF to Arrow Streams and Duckdb `bcf_reader` extension

RBCFTools provides streaming VCF/BCF to Apache Arrow Stream conversion via [nanoarrow](https://arrow.apache.org/nanoarrow/). This enables integration with tools like [duckdb](https://github.com/duckdb/duckdb-r) and Parquet format convertion when serializing to Arrow IPC. We also support the [`bcf_reader`](inst/duckdb_bcf_reader_extension/) duckdb extension to read directly into duckb

The `nanoarrow` stream conversion and `bcf_reader` perform VCF spec conformance checks on headers (similar to htslib's `bcf_hdr_check_sanity()`) and emits R warning or duckdb logs when correcting non-conformant fields


### Read VCF as Arrow Stream

Open BCF as Arrow array stream and read the batches


```{r arrow-stream, eval=TRUE}

bcf_file <- system.file("extdata", "1000G_3samples.bcf", package = "RBCFTools")
stream <- vcf_open_arrow(bcf_file, batch_size = 100L)

batch <- stream$get_next()

nanoarrow::convert_array(batch)
stream$release()
```

### Convert to Data Frame

Convert entire BCF to data.frame

```{r arrow-df, eval=TRUE}
options(warn = -1)  # Suppress warnings
df <- vcf_to_arrow(bcf_file, as = "data.frame")
df[, c("CHROM", "POS", "REF", "ALT", "QUAL")] |> head()

```


### Write to Arrow IPC

Arrow IPC (`.arrows`) format for interoperability with other Arrow tools using nanoarrow's native streaming writer 

```{r arrow-ipc, eval=TRUE}

# Convert BCF to Arrow IPC
ipc_file <- tempfile(fileext = ".arrows")

vcf_to_arrow_ipc(bcf_file, ipc_file)

# Read back with nanoarrow
ipc_data <- as.data.frame(nanoarrow::read_nanoarrow(ipc_file))
ipc_data[, c("CHROM", "POS", "REF", "ALT")] |> head()
```

### Write VCF Streams to Parquet

Using [duckdb](https://github.com/duckdb/duckdb-r)  to convert BCF to parquet file and perform queries on the parquet file. This involve vcf stream conversion to data.frame

```{r parquet, eval=TRUE}

parquet_file <- tempfile(fileext = ".parquet")
vcf_to_parquet(bcf_file, parquet_file, compression = "snappy")
con <- duckdb::dbConnect(duckdb::duckdb())
pq_bcf <- DBI::dbGetQuery(con, sprintf("SELECT * FROM '%s' LIMIT 100", parquet_file))
pq_me <- DBI::dbGetQuery(
    con, 
    sprintf("SELECT * FROM parquet_metadata('%s')",
    parquet_file))
duckdb::dbDisconnect(con, shutdown = TRUE)
pq_bcf[, c("CHROM", "POS", "REF", "ALT")] |>
  head()
pq_me |> head()
```

Use `streaming = TRUE` to avoid loading the entire VCF into R memory. This streams VCF to Arrow IPC (nanoarrow) and then to Parquet via duckdb, trading memory with serialization overhead using the [duckdb nanoarrow extension](https://duckdb.org/community_extensions/extensions/nanoarrow.html)

```{r streaming, eval = TRUE}
bcf_larger <- system.file("extdata", "1000G.ALL.2of4intersection.20100804.genotypes.bcf", package = "RBCFTools")
outfile <-  tempfile(fileext = ".parquet")
vcf_to_parquet(
    bcf_larger,
    outfile,
    streaming = TRUE,
    batch_size = 10000L,
    row_group_size = 100000L,
    compression = "zstd"
)
# describe using duckdb

```

### Query VCF with duckdb after converting the Stream

SQL queries on BCF using duckdb package. For now this is somehow limited due to convertion from arrow streams to data frame

```{r duckdb, eval=TRUE}
vcf_query(bcf_file, "SELECT CHROM, COUNT(*) as n FROM vcf GROUP BY CHROM")

# Filter variants by position
vcf_query(bcf_file, "SELECT CHROM, POS, REF, ALT FROM vcf  LIMIT 5")
```

###  Query VCF with  DuckDB Extension

A native DuckDB extension (`bcf_reader`) for direct SQL queries on VCF/BCF files without Arrow conversion overhead. The extension uses the Duckb C API and should be compatible with duckb v1.2.0+

```{r duckdb-ext, eval=TRUE}
# Build extension (uses package's bundled htslib)
build_dir <- file.path(tempdir(), "bcf_reader")
ext_path <- bcf_reader_build(build_dir, verbose = FALSE)

# Connect and query a VCF.gz file
con <- vcf_duckdb_connect(ext_path)
vcf_file <- system.file("extdata", "test_deep_variant.vcf.gz", package = "RBCFTools")

# Describe schema
DBI::dbGetQuery(con, sprintf("DESCRIBE SELECT * FROM bcf_read('%s')", vcf_file))

# Aggregate query
DBI::dbGetQuery(con, sprintf("
  SELECT CHROM, COUNT(*) as n_variants, 
         MIN(POS) as min_pos, MAX(POS) as max_pos
  FROM bcf_read('%s') 
  GROUP BY CHROM 
  ORDER BY n_variants DESC 
  LIMIT 5", vcf_file))

# Export directly to Parquet
parquet_out <- tempfile(fileext = ".parquet")
DBI::dbExecute(con, sprintf("
  COPY (SELECT * FROM bcf_read('%s')) 
  TO '%s' (FORMAT PARQUET, COMPRESSION ZSTD)", vcf_file, parquet_out))

# Verify parquet file size
file.info(parquet_out)$size

# Same query on Parquet file
DBI::dbGetQuery(con, sprintf("
  SELECT CHROM, COUNT(*) as n_variants, 
         MIN(POS) as min_pos, MAX(POS) as max_pos
  FROM '%s' 
  GROUP BY CHROM 
  ORDER BY n_variants DESC 
  LIMIT 5", parquet_out))

DBI::dbDisconnect(con)
```

### Stream Remote VCF using Arrow or DuckDB Extension

Stream remote VCF region directly from S3 using either `nanoarrow` based `vcf_stream` or `duckb` extension

```{r url-arrow, eval=TRUE}
s3_vcf_uri <- paste0(s3_base, s3_path, s3_vcf_file)

# Arrow stream
stream <- vcf_open_arrow(
    s3_vcf_uri,
    region = "chr22:16050000-16050500",
    batch_size = 1000L
)
df <- as.data.frame(nanoarrow::convert_array_stream(stream))
df[, c("CHROM", "POS", "REF", "ALT")] |> head()


# Query remote VCF with bcf_reader extension

vcf_query_duckdb(
    s3_vcf_uri,
    ext_path,
    region = "chr22:16050000-16050500",
    query = "SELECT CHROM, POS, REF, ALT FROM vcf LIMIT 5"
)


```

### Command-Line Tool

A CLI tool is provided for VCF to Parquet conversion and querying, including threaded chunking based on contigs

```{bash cli-tool, eval=TRUE}
# Get paths using system.file
SCRIPT=$(Rscript -e "cat(system.file('scripts', 'vcf2parquet.R', package='RBCFTools'))")
BCF=$(Rscript -e "cat(system.file('extdata', '1000G_3samples.bcf', package='RBCFTools'))")
OUT_PQ=$(mktemp --suffix=.parquet)

# Convert BCF to Parquet
$SCRIPT convert --quiet -i $BCF -o $OUT_PQ

# Query with DuckDB SQL
$SCRIPT query -i $OUT_PQ -q "SELECT CHROM, POS, REF, ALT FROM parquet_scan('$OUT_PQ') LIMIT 5"

# Describe table structure
$SCRIPT query -i $OUT_PQ -q "DESCRIBE SELECT * FROM parquet_scan('$OUT_PQ')"

# Show schema
$SCRIPT schema 0 --quiet -i $BCF

# File info
$SCRIPT info -i $OUT_PQ

rm -f $OUT_PQ
```



### Custom Index File Path

For region queries, an index file is required. By default, RBCFTools looks for `.tbi` (tabix) or `.csi` indexes using standard naming conventions. For non-standard index locations such as presigned URLs or custom paths, use the `index` parameter. Index files are only required for region queries; when reading an entire file without a region filter, no index is needed. VCF files try `.tbi` first then fall back to `.csi`, while BCF files use `.csi` only.

```{r index-param, eval=TRUE}
# Explicit index file path
bcf_file <- system.file("extdata", "1000G_3samples.bcf", package = "RBCFTools")
csi_index <- system.file("extdata", "1000G_3samples.bcf.csi", package = "RBCFTools")

stream <- vcf_open_arrow(bcf_file, index = csi_index)
batch <- stream$get_next()
nanoarrow::convert_array(batch)[, c("CHROM", "POS", "REF")] |> head(3)

# Alternative: htslib ##idx## syntax in filename
filename_with_idx <- paste0(bcf_file, "##idx##", csi_index)
stream2 <- vcf_open_arrow(filename_with_idx)
batch2 <- stream2$get_next()
nanoarrow::convert_array(batch2)[, c("CHROM", "POS", "REF")] |> head(3)
```

## Limitations and issues

Arrow stream still copies at the C level and again on Parquet conversion (Arrow IPC serialization); optimizing zero-copy paths remains open. Remaining structured annotation formats (e.g., SnpEff extras) are not parsed beyond VCF header types. The duckdb extension can be improve by pushing down further for indexed files.

## Future directions

We should improve the code, avoid copies, add more tests and maybe [ducklake](https://github.com/duckdb/ducklake) support


## References

- [bcftools documentation](https://samtools.github.io/bcftools/)

- [bcftools GitHub](https://github.com/samtools/bcftools)

- [htslib GitHub](https://github.com/samtools/htslib)

- [arrow-nanoarrow](https://arrow.apache.org/nanoarrow/)
