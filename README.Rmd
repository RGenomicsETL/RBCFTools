---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# RBCFTools

<!-- badges: start -->
[![R-CMD-check](https://github.com/RGenomicsETL/RBCFTools/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/RGenomicsETL/RBCFTools/actions/workflows/R-CMD-check.yaml)
[![R-universe version](https://RGenomicsETL.r-universe.dev/RBCFTools/badges/version)](https://RGenomicsETL.r-universe.dev/RBCFTools)
<!-- badges: end -->

RBCFTools provides R bindings to [bcftools](https://github.com/samtools/bcftools) and [htslib](https://github.com/samtools/htslib), the standard tools for reading and manipulating VCF/BCF files. The package bundles these libraries and command-line tools (bcftools, bgzip, tabix), so no external installation is required. When compiled with libcurl, remote file access from S3, GCS, and HTTP URLs is supported. The package also includes support for streaming VCF/BCF to Apache Arrow (IPC) format via [nanoarrow](https://arrow.apache.org/nanoarrow/), with export to Parquet format using [duckdb](https://duckdb.org/) via either the  [duckdb nanoarrow extension](https://duckdb.org/community_extensions/extensions/nanoarrow.html) or a [`bcf_reader`](inst/duckdb_bcf_reader_extension/) extension bundled in this package.

## Installation

You can install the development version of RBCFTools from r-universe for unix-alikes (we do not support windows)
 
```r
install.packages(
    'RBCFTools',
    repos = c(
              'https://rgenomicsetl.r-universe.dev',
              'https://cloud.r-project.org')
      )
```

## Version Information

```{r versions}
library(RBCFTools)
# Get library versions
bcftools_version()
htslib_version()
```

## Tool Paths

RBCFTools bundles bcftools and htslib command-line tools. Use the path functions to locate the executables

```{r paths}
bcftools_path()
bgzip_path()
tabix_path()
# List all available tools
bcftools_tools()
htslib_tools()
```

## Capabilities

Check which features were compiled into htslib

```{r capabilities}
# Get all capabilities as a named list
htslib_capabilities()

# Human-readable feature string
htslib_feature_string()
```



### Feature Constants

Use `HTS_FEATURE_*` constants to check for specific features
 
```{r features}
# Check individual features
htslib_has_feature(HTS_FEATURE_LIBCURL)  # Remote file access via libcurl
htslib_has_feature(HTS_FEATURE_S3)       
htslib_has_feature(HTS_FEATURE_GCS)      # Google Cloud Storage
htslib_has_feature(HTS_FEATURE_LIBDEFLATE)
htslib_has_feature(HTS_FEATURE_LZMA)
htslib_has_feature(HTS_FEATURE_BZIP2)
```

These are useful for conditionally enabling features in your code.

## Example Query Of Remote VCF from S3 using bcftools

With libcurl support, bcftools can directly query remote files. Here we count variants in a small region from the 1000 Genomes cohort VCF on S3:

```{r s3 setup, eval=TRUE}
# Setup environment for remote file access (S3/GCS)
setup_hts_env()

# Build S3 URL for 1000 Genomes cohort VCF
s3_base <- "s3://1000genomes-dragen-v3.7.6/data/cohorts/"
s3_path <- "gvcf-genotyper-dragen-3.7.6/hg19/3202-samples-cohort/"
s3_vcf_file <- "3202_samples_cohort_gg_chr22.vcf.gz"
s3_vcf_uri <- paste0(s3_base, s3_path, s3_vcf_file)
```

```{r s3-example, eval=TRUE}
# Use processx instead of system2
res <- processx::run(
  bcftools_path(),
  c("index", "-n", s3_vcf_uri),
  error_on_status = FALSE,
  echo = FALSE
)
result <- strsplit(res$stdout, "\n")[[1]]
length(result[result != ""])
```

##  VCF to Arrow Streams and Duckdb `bcf_reader` extension



RBCFTools provides streaming VCF/BCF to Apache Arrow Stream conversion via [nanoarrow](https://arrow.apache.org/nanoarrow/). This enables integration with tools like [duckdb](https://github.com/duckdb/duckdb-r) and Parquet format convertion when serializing to Arrow IPC. We also support the [`bcf_reader`](inst/duckdb_bcf_reader_extension/) duckdb extension to read directly into duckb.

The `nanoarrow` stream conversion and `bcf_reader` perform VCF spec conformance checks on headers (similar to htslib's `bcf_hdr_check_sanity()`) and emits R warning or duckdb logs when correcting non-conformant fields


### Read VCF as Arrow Stream

Open BCF as Arrow array stream and read the batches


```{r arrow-stream, eval=TRUE}

bcf_file <- system.file("extdata", "1000G_3samples.bcf", package = "RBCFTools")
stream <- vcf_open_arrow(bcf_file, batch_size = 100L)

batch <- stream$get_next()

nanoarrow::convert_array(batch)
stream$release()
```

### Convert to Data Frame

Convert entire BCF to data.frame

```{r arrow-df, eval=TRUE}
options(warn = -1)  # Suppress warnings
df <- vcf_to_arrow(bcf_file, as = "data.frame")
df[, c("CHROM", "POS", "REF", "ALT", "QUAL")] |> head()

```


### Write to Arrow IPC

Arrow IPC (`.arrows`) format for interoperability with other Arrow tools using nanoarrow's native streaming writer 

```{r arrow-ipc, eval=TRUE}

# Convert BCF to Arrow IPC
ipc_file <- tempfile(fileext = ".arrows")

vcf_to_arrow_ipc(bcf_file, ipc_file)

# Read back with nanoarrow
ipc_data <- as.data.frame(nanoarrow::read_nanoarrow(ipc_file))
ipc_data[, c("CHROM", "POS", "REF", "ALT")] |> head()
```

### Write VCF Streams to Parquet

Using [duckdb](https://github.com/duckdb/duckdb-r)  to convert BCF to parquet file and perform queries on the parquet file. This involve vcf stream conversion to data.frame

```{r parquet, eval=TRUE}

parquet_file <- tempfile(fileext = ".parquet")
vcf_to_parquet_arrow(bcf_file, parquet_file, compression = "snappy")
con <- duckdb::dbConnect(duckdb::duckdb())
pq_bcf <- DBI::dbGetQuery(con, sprintf("SELECT * FROM '%s' LIMIT 100", parquet_file))
pq_me <- DBI::dbGetQuery(
    con, 
    sprintf("SELECT * FROM parquet_metadata('%s')",
    parquet_file))
duckdb::dbDisconnect(con, shutdown = TRUE)
pq_bcf[, c("CHROM", "POS", "REF", "ALT")] |>
  head()
pq_me |> head()
```

Use `streaming = TRUE` to avoid loading the entire VCF into R memory. This streams VCF to Arrow IPC (nanoarrow) and then to Parquet via duckdb, trading memory with serialization overhead using the [duckdb nanoarrow extension](https://duckdb.org/community_extensions/extensions/nanoarrow.html)

```{r streaming, eval = TRUE}
bcf_larger <- system.file("extdata", "1000G.ALL.2of4intersection.20100804.genotypes.bcf", package = "RBCFTools")
outfile <-  tempfile(fileext = ".parquet")
vcf_to_parquet_arrow(
    bcf_larger,
    outfile,
    streaming = TRUE,
    batch_size = 10000L,
    row_group_size = 100000L,
    compression = "zstd"
)
# describe using duckdb

```

### Query VCF with duckdb after converting the Stream

SQL queries on BCF using duckdb package. For now this is somehow limited due to convertion from arrow streams to data frame

```{r duckdb, eval=TRUE}
vcf_query_arrow(bcf_file, "SELECT CHROM, COUNT(*) as n FROM vcf GROUP BY CHROM")

# Filter variants by position
vcf_query_arrow(bcf_file, "SELECT CHROM, POS, REF, ALT FROM vcf  LIMIT 5")
```

###  Query VCF with  DuckDB Extension

A native DuckDB extension (`bcf_reader`) for direct SQL queries on VCF/BCF files without Arrow conversion overhead. The extension uses the Duckb C API and should be compatible with duckb v1.2.0+

```{r duckdb-ext, eval=TRUE}
# Build extension (uses package's bundled htslib)
build_dir <- file.path(tempdir(), "bcf_reader")
ext_path <- bcf_reader_build(build_dir, verbose = FALSE)

# Connect and query a VCF.gz file
con <- vcf_duckdb_connect(ext_path)
vcf_file <- system.file("extdata", "test_deep_variant.vcf.gz", package = "RBCFTools")

# Describe schema
DBI::dbGetQuery(con, sprintf("DESCRIBE SELECT * FROM bcf_read('%s')", vcf_file))

# Aggregate query
DBI::dbGetQuery(con, sprintf("
  SELECT CHROM, COUNT(*) as n_variants, 
         MIN(POS) as min_pos, MAX(POS) as max_pos
  FROM bcf_read('%s') 
  GROUP BY CHROM 
  ORDER BY n_variants DESC 
  LIMIT 5", vcf_file))

# Export directly to Parquet
parquet_out <- tempfile(fileext = ".parquet")
DBI::dbExecute(con, sprintf("
  COPY (SELECT * FROM bcf_read('%s')) 
  TO '%s' (FORMAT PARQUET, COMPRESSION ZSTD)", vcf_file, parquet_out))

# Verify parquet file size
file.info(parquet_out)$size

# Same query on Parquet file
DBI::dbGetQuery(con, sprintf("
  SELECT CHROM, COUNT(*) as n_variants, 
         MIN(POS) as min_pos, MAX(POS) as max_pos
  FROM '%s' 
  GROUP BY CHROM 
  ORDER BY n_variants DESC 
  LIMIT 5", parquet_out))

DBI::dbDisconnect(con)
```



### Stream Remote VCF using Arrow or DuckDB Extension

Stream remote VCF region directly from S3 using either `nanoarrow` based `vcf_stream` or `duckb` extension

```{r url-arrow, eval=TRUE}
s3_vcf_uri <- paste0(s3_base, s3_path, s3_vcf_file)

# Arrow stream
stream <- vcf_open_arrow(
    s3_vcf_uri,
    region = "chr22:16050000-16050500",
    batch_size = 1000L
)
df <- as.data.frame(nanoarrow::convert_array_stream(stream))
df[, c("CHROM", "POS", "REF", "ALT")] |> head()


# Query remote VCF with bcf_reader extension

vcf_query_duckdb(
    s3_vcf_uri,
    ext_path,
    region = "chr22:16050000-16050500",
    query = "SELECT CHROM, POS, REF, ALT FROM vcf LIMIT 5"
)


```
### Custom Index File Path

For region queries, an index file is required. By default, RBCFTools looks for `.tbi` (tabix) or `.csi` indexes using standard naming conventions. For non-standard index locations such as presigned URLs or custom paths, use the `index` parameter. Index files are only required for region queries; when reading an entire file without a region filter, no index is needed. VCF files try `.tbi` first then fall back to `.csi`, while BCF files use `.csi` only.

```{r index-param, eval=TRUE}
# Explicit index file path
bcf_file <- system.file("extdata", "1000G_3samples.bcf", package = "RBCFTools")
csi_index <- system.file("extdata", "1000G_3samples.bcf.csi", package = "RBCFTools")

stream <- vcf_open_arrow(bcf_file, index = csi_index)
batch <- stream$get_next()
nanoarrow::convert_array(batch)[, c("CHROM", "POS", "REF")] |> head(3)

# Alternative: htslib ##idx## syntax in filename
filename_with_idx <- paste0(bcf_file, "##idx##", csi_index)
stream2 <- vcf_open_arrow(filename_with_idx)
batch2 <- stream2$get_next()
nanoarrow::convert_array(batch2)[, c("CHROM", "POS", "REF")] |> head(3)
```

## Example Application: DuckLake ETL

[`DuckLake`](https://ducklake.select/) is an integrated data lake and catalog format. It has two components, a parquet files storage and a metadata dababase layer.

### DuckLake ETL to MinIO Storage backend

To mimick a S3 backend for [`DuckLake`](https://ducklake.select/) storage we use [minio](github.com/minio/minio), we convert VCF files to parquet and insert then into DuckLake and query the lake. The lake as a local duckdb metadata database in this example


#### Setup up `minio` storage backend
We start by seting up and configuring a `minio` server

```{r ducklake-minio, eval=TRUE}
# DuckLake with S3-compatible storage using local MinIO
if (!requireNamespace("processx", quietly = TRUE)) stop("processx required")
bin_dir <- file.path(tempdir(), "ducklake_bins")
dir.create(bin_dir, recursive = TRUE, showWarnings = FALSE)

# Use installed MinIO and MC binaries
minio_bin <- Sys.which('minio')
mc_bin <- Sys.which('mc')

# Start MinIO (ephemeral) and configure mc
data_dir <- file.path(tempdir(), "ducklake_minio")
dir.create(data_dir, recursive = TRUE, showWarnings = FALSE)
port <- 9000
endpoint <- sprintf("127.0.0.1:%d", port)

# Start MinIO server in background
cmd <- sprintf(
  "%s server %s --address %s > /dev/null 2>&1 & echo $!",
  shQuote(minio_bin),
  shQuote(data_dir),
  endpoint
)
pid_output <- processx::run("sh", c("-c", cmd), echo = FALSE)$stdout
pid <- as.integer(pid_output)
pid
# Give MinIO time to start
Sys.sleep(10)

# Configure mc alias
# remove previous alias
processx::run(
  mc_bin,
  c("alias", "remove", "ducklake_local"),
  error_on_status = FALSE,
  echo = FALSE
)
mc_cmd_args <- c("alias", "set", "ducklake_local", 
                  paste0("http://", endpoint), "minioadmin", "minioadmin")
processx::run(mc_bin, mc_cmd_args, echo = FALSE)

# Create bucket with unique name
bucket <- sprintf("readme-demo-%d", as.integer(Sys.time()))
bucket_cmd_args <- c("mb", paste0("ducklake_local/", bucket))
processx::run(mc_bin, bucket_cmd_args, echo = FALSE)

# Store variables for later use
minio_endpoint <- endpoint
bucket_name <- bucket
data_root_s3 <- paste0("s3://", bucket_name)
data_path_s3 <- paste0(data_root_s3, "/data/")
mc_path <- mc_bin
```

####  Attach a DuckLake

```{r ducklake-attach, eval=TRUE}
con <- duckdb::dbConnect(duckdb::duckdb(config = list(
  allow_unsigned_extensions = "true",
  enable_external_access = "true"
)))
stopifnot(DBI::dbIsValid(con))

DBI::dbExecute(con, "INSTALL httpfs")
DBI::dbExecute(con, "LOAD httpfs")
DBI::dbExecute(con, "INSTALL ducklake FROM core_nightly")
DBI::dbExecute(con, "LOAD ducklake")

# Provide S3 credentials/endpoints for MinIO
Sys.setenv(
  AWS_ACCESS_KEY_ID = "minioadmin",
  AWS_SECRET_ACCESS_KEY = "minioadmin",
  AWS_DEFAULT_REGION = "us-east-1",
  AWS_S3_ENDPOINT = paste0("http://", minio_endpoint),
  AWS_S3_USE_HTTPS = "FALSE",
  AWS_S3_FORCE_PATH_STYLE = "TRUE",
  AWS_EC2_METADATA_DISABLED = "TRUE"
)

ducklake_create_s3_secret(
  con,
  name = "ducklake_minio",
  key_id = "minioadmin",
  secret = "minioadmin",
  endpoint = minio_endpoint,
  region = "us-east-1",
  use_ssl = FALSE
)

metadata_path <- file.path(tempdir(), sprintf("ducklake_meta_%s.ducklake", bucket_name))
ducklake_attach(
  con,
  metadata_path = metadata_path,
  data_path = data_path_s3,
  alias = "lake",
  extra_options = list(OVERRIDE_DATA_PATH = TRUE)
)

# Work inside the attached lake database
DBI::dbExecute(con, "USE lake")
```

### Write a VCF into minio and register to the Lake
```{r ducklake-write, eval=TRUE}
# Ensure we are operating inside the DuckLake catalog
DBI::dbExecute(con, "USE lake")

# Load variants via fast VCF to Parquet conversion
vcf_file <- system.file("extdata", "test_deep_variant.vcf.gz", package = "RBCFTools")
ext_path <- bcf_reader_build(tempdir())
ducklake_load_vcf(
  con,
  table = "variants",
  vcf_path = vcf_file,
  extension_path = ext_path,
  threads = 1
)
```

### List lake content
```{r check-lake, eval=TRUE}
DBI::dbExecute(con, "USE lake")
DBI::dbGetQuery(con, "SELECT COUNT(*) AS n FROM lake.variants")
```

```{r ducklake-files, eval=TRUE}
# List physical files managed by DuckLake for this table
DBI::dbGetQuery(con, "FROM ducklake_list_files('lake', 'variants')")
DBI::dbDisconnect(con, shutdown = TRUE)
tools::pskill(pid)
```

### Supported Metadata Databases

DuckLake supports multiple catalog backends

- `DuckDB` : `ducklake:path/to/catalog.ducklake`
- `SQLite`: `ducklake:sqlite://path/to/catalog.db`  
- `PostgreSQL` : `ducklake:postgresql://user:pass@host:5432/db`
- `MySQL`: `ducklake:mysql://user:pass@host:3306/db`

#### Connection Methods

The package provides helpers for different metadata databases

#### Direct Connection

```r
# DuckDB backend
ducklake_connect_catalog(
  con,
  backend = "duckdb",
  connection_string = "catalog.ducklake",
  data_path = "s3://bucket/data/",
  alias = "lake"
)

# PostgreSQL backend
ducklake_connect_catalog(
  con,
  backend = "postgresql", 
  connection_string = "user:pass@host:5432/db",
  data_path = "s3://bucket/data/",
  alias = "lake"
)
```

##### Secret-based Connection:**
```r
# Create catalog secret
ducklake_create_catalog_secret(
  con,
  name = "pg_catalog",
  backend = "postgresql",
  connection_string = "user:pass@host:5432/db",
  data_path = "s3://bucket/data/"
)

# Connect using secret
ducklake_connect_catalog(
  con,
  secret_name = "pg_catalog",
  alias = "lake"
)
```

## Command-Line utilities

CLI tools are provided for VCF to Parquet conversion and querying, including threaded chunking based on contigs using either the [`bcf_reader`](./inst/scripts/vcf2parquet_duckdb.R) extension or via [Arrow IPC](./inst/scripts/vcf2parquet.R) 

```{bash cli-tool, eval=TRUE}
# Get paths using system.file
SCRIPT=$(Rscript -e "cat(system.file('scripts', 'vcf2parquet_duckdb.R', package='RBCFTools'))")
BCF=$(Rscript -e "cat(system.file('extdata', 'test_deep_variant.vcf.gz', package='RBCFTools'))")
OUT_PQ=$(mktemp --suffix=.parquet)
Log=$(mktemp --suffix=.log)

# Convert BCF to Parquet
time $SCRIPT convert --quiet -i $BCF -o $OUT_PQ -t 4  > $Log 2>&1

cat $Log

# Query with DuckDB SQL
$SCRIPT query -i $OUT_PQ -q "SELECT CHROM, POS, REF, ALT FROM parquet_scan('$OUT_PQ') LIMIT 5"

# Describe table structure
$SCRIPT query -i $OUT_PQ -q "DESCRIBE SELECT * FROM parquet_scan('$OUT_PQ')"

# Show schema
$SCRIPT schema  --quiet -i $BCF 

# File info
$SCRIPT info -i $OUT_PQ 

rm -f $OUT_PQ
```



## References

- [bcftools documentation](https://samtools.github.io/bcftools/)

- [bcftools GitHub](https://github.com/samtools/bcftools)

- [htslib GitHub](https://github.com/samtools/htslib)

- [arrow-nanoarrow](https://arrow.apache.org/nanoarrow/)

- [Ducklake minio Example](https://github.com/duckdb/ducklake/blob/main/examples/minio-demo-server/README.md)
