% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/vcf_duckdb.R
\name{vcf_to_parquet_duckdb_chunked}
\alias{vcf_to_parquet_duckdb_chunked}
\title{Convert VCF/BCF to Parquet using adaptive chunking strategy}
\usage{
vcf_to_parquet_duckdb_chunked(
  input_file,
  output_file,
  extension_path = NULL,
  columns = NULL,
  compression = "zstd",
  row_group_size = 1e+05,
  threads = parallel::detectCores(),
  contigs = NULL
)
}
\arguments{
\item{input_file}{Path to input VCF, VCF.GZ, or BCF file}

\item{output_file}{Path to output Parquet file}

\item{extension_path}{Path to the bcf_reader.duckdb_extension file.}

\item{columns}{Optional character vector of columns to include. NULL for all.}

\item{compression}{Parquet compression: "snappy", "zstd", "gzip", or "none"}

\item{row_group_size}{Number of rows per row group (default: 100000)}

\item{threads}{Number of parallel threads for processing (default: 1).
When threads > 1 and file is indexed, uses parallel processing by splitting
work across chromosomes/contigs. See \code{\link{vcf_to_parquet_duckdb_parallel}}.}

\item{contigs}{Character vector of contig names}
}
\value{
List with conversion metrics
}
\description{
Uses sub-chromosomal chunking for optimal parallelization.
Works for both single and multi-chromosome files by creating
~100MB chunks across all chromosomes.
}
\keyword{internal}
