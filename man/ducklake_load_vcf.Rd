% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ducklake.R
\name{ducklake_load_vcf}
\alias{ducklake_load_vcf}
\title{Load VCF into DuckLake (ETL + Registration)}
\usage{
ducklake_load_vcf(
  con,
  table,
  vcf_path,
  extension_path,
  output_path = NULL,
  threads = parallel::detectCores(),
  compression = "zstd",
  row_group_size = 100000L,
  region = NULL,
  columns = NULL,
  overwrite = FALSE,
  allow_evolution = FALSE,
  tidy_format = FALSE,
  partition_by = NULL
)
}
\arguments{
\item{con}{DuckDB connection with DuckLake attached.}

\item{table}{Target table name (optionally qualified, e.g., "lake.variants").}

\item{vcf_path}{Path/URI to VCF/BCF file.}

\item{extension_path}{Path to bcf_reader.duckdb_extension (required).}

\item{output_path}{Optional Parquet output path. If NULL, uses DuckLake's DATA_PATH.}

\item{threads}{Number of threads for conversion.}

\item{compression}{Parquet compression codec.}

\item{row_group_size}{Parquet row group size.}

\item{region}{Optional region filter (e.g., "chr1:1000-2000").}

\item{columns}{Optional character vector of columns to include.}

\item{overwrite}{Logical, drop existing table first.}

\item{allow_evolution}{Logical, evolve table schema by adding new columns from VCF.
Default: FALSE. When TRUE, new columns found in the VCF are added via ALTER TABLE
before insertion, making all columns queryable. Useful for combining VCFs with
different annotations (e.g., VEP columns) or different samples (FORMAT_*_SampleName).}

\item{tidy_format}{Logical, if TRUE exports data in tidy (long) format with one
row per variant-sample combination and a SAMPLE_ID column. Default FALSE.
Ideal for cohort analysis and combining multiple single-sample VCFs.}

\item{partition_by}{Optional character vector of columns to partition by (Hive-style).
Creates directory structure like \code{output_dir/SAMPLE_ID=HG00098/data_0.parquet}.
Note: DuckLake registration currently requires single Parquet files; when using
partition_by, the output_path should point to the partition directory and
files should be registered separately.}
}
\value{
Invisibly returns the path to the created Parquet file.
}
\description{
Converts VCF/BCF to Parquet using the fast \code{bcf_reader} extension, then
registers the Parquet file in a DuckLake catalog table.
}
\details{
This is the recommended function for loading VCF data into DuckLake.
It uses the \code{bcf_reader} DuckDB extension for fast VCF→Parquet conversion,
which is significantly faster than the nanoarrow streaming path.

\strong{Workflow:}
\enumerate{
\item VCF → Parquet via \code{vcf_to_parquet_duckdb()} (bcf_reader)
\item Register Parquet in DuckLake catalog
}

\strong{Schema Evolution (\code{allow_evolution = TRUE}):}
When loading multiple VCFs with different schemas (e.g., different samples
or different annotation fields), enable \code{allow_evolution} to automatically
add new columns to the table schema. This uses DuckLake's \verb{ALTER TABLE ADD COLUMN}
which preserves existing data files without rewriting.

\strong{Tidy Format (\code{tidy_format = TRUE}):}
When building cohort tables from multiple single-sample VCFs, use \code{tidy_format = TRUE}
to get one row per variant-sample combination with a \code{SAMPLE_ID} column. This format
is ideal for downstream analysis and MERGE/UPSERT operations on DuckLake tables.

\strong{Partitioning (\code{partition_by}):}
When using \code{partition_by}, the output is a Hive-partitioned directory structure.
This is useful for large cohorts where you want efficient per-sample queries.
DuckDB auto-generates Bloom filters for VARCHAR columns like SAMPLE_ID.
Note: For DuckLake, partitioned output requires manual file registration.
}
\examples{
\dontrun{
# Build extension
ext_path <- bcf_reader_build(tempdir())

# Setup DuckLake
con <- duckdb::dbConnect(duckdb::duckdb())
ducklake_load(con)
ducklake_attach(con, "catalog.ducklake", "/data/parquet/", alias = "lake")
DBI::dbExecute(con, "USE lake")

# Load first VCF
ducklake_load_vcf(con, "variants", "sample1.vcf.gz", ext_path, threads = 8)

# Load second VCF with different annotations, evolving schema
ducklake_load_vcf(con, "variants", "sample2_vep.vcf.gz", ext_path,
  allow_evolution = TRUE
)

# Load VCF in tidy format (one row per variant-sample)
ducklake_load_vcf(con, "variants_tidy", "cohort.vcf.gz", ext_path,
  tidy_format = TRUE
)

# Query - all columns from both VCFs are available
DBI::dbGetQuery(con, "SELECT CHROM, COUNT(*) FROM variants GROUP BY CHROM")
}
}
